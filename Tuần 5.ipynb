{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model nhận diện sử dụng Mediapipe\n",
    "\n",
    "### Tìm hiểu về cách nhận diện sử dụng Mediapipe\n",
    "\n",
    "Một số video đã xem:\n",
    "\n",
    "Video **American Sign Language Detection with Python and MediaPipe** của **Rob Mulla. Link video:** https://www.youtube.com/watch?v=L-IaQch8KYY\n",
    "\n",
    "Video **Sign Language Detection Using Machine Learning | Python Project** của **Progress With Python. Link video:** https://www.youtube.com/watch?v=Ui85SVJsRf8\n",
    "\n",
    "Video **Sign language detection with Python and Scikit Learn | Landmark detection | Computer vision tutorial** của **Computer vision engineer. Link video:** https://www.youtube.com/watch?v=MJCSjXepaAM\n",
    "\n",
    "Video **Custom Hand Gesture Recognition with Hand Landmarks Using Google’s Mediapipe + OpenCV in Python của Ivan Goncharov. Link video:** https://www.youtube.com/watch?v=a99p_fAr6e4\n",
    "\n",
    "### Thực hiện train model theo video **Hands gesture recognition with mediapipe and svm classifier của AI4LIFE**\n",
    "\n",
    "**Link Video:** [Hands gesture recognition with mediapipe and svm classifier](https://www.youtube.com/watch?v=dlyVy_LNCEQ)\n",
    "\n",
    "**Link Github:** [hand-gesture-recognition](https://github.com/dongdv95/hand-gesture-recognition)\n",
    "\n",
    "Sau khi xem một vài video hướng dẫn nhận diện tay sử dụng Mediapipe thì em thử thực hiện model với video hướng dẫn này. Kết quả thu được model nhận diện tay có độ chính xác còn thấp.\n",
    "\n",
    "File **get_image.py** dùng để thực hiện thu thập hình ảnh cho dataset với mỗi ký tự là 100 ảnh, 50 ảnh cho tay trái và 50 ảnh cho tay phải( trừ chữ A, B, C, D, E, F, G, H). Sau quá trình đó thì ta thu được một DATASET với 2600 ảnh.\n",
    "\n",
    "File **get_data.py** dùng để thực hiện label tự động cho mỗi ảnh để thu được các landmark của từng ảnh và được tập hợp lại thành file **dataset.csv**.\n",
    "\n",
    "File **train.ipynb** dùng để thực hiện train model là **model.pkl**.\n",
    "\n",
    "Sau khi có model thực hiện chạy wecam detect với file **hands_gesture_recog.py**. Vì em cảm thấy thiếu Landmark Mediapipe khi chạy wecam để kiểm tra detect tay khi sử dụng Mediapipe thế nào nên em đã lên chatGPT để thêm vào code hiện thị Landmark khi chạy wecam.\n",
    "\n",
    "Code:\n",
    "\n",
    "```\n",
    "        import cv2\n",
    "        import mediapipe as mp\n",
    "        import numpy as np\n",
    "        import pickle\n",
    "\n",
    "        # Initialize MediaPipe Hands module and drawing utilities\n",
    "        mp_hands = mp.solutions.hands\n",
    "        mp_drawing = mp.solutions.drawing_utils\n",
    "        hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)\n",
    "\n",
    "        def image_processed(hand_img):\n",
    "            \"\"\"\n",
    "            This function processes the input image and extracts hand landmarks\n",
    "            using MediaPipe's Hand Landmark detection model.\n",
    "            \"\"\"\n",
    "            # Convert BGR to RGB\n",
    "            img_rgb = cv2.cvtColor(hand_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Flip the image for mirror effect (optional)\n",
    "            # img_flip = cv2.flip(img_rgb, 1)\n",
    "\n",
    "            # Process the flipped RGB image with MediaPipe Hands\n",
    "            output = hands.process(img_rgb)\n",
    "\n",
    "            try:\n",
    "                # Extract hand landmarks if present\n",
    "                data = output.multi_hand_landmarks[0]\n",
    "                data = str(data).strip().split('\\n')\n",
    "\n",
    "                # Filter out unnecessary data\n",
    "                garbage = ['landmark {', '  visibility: 0.0', '  presence: 0.0', '}']\n",
    "                without_garbage = [i.strip()[2:] for i in data if i not in garbage]\n",
    "\n",
    "                # Convert to float for model compatibility\n",
    "                clean = [float(i) for i in without_garbage]\n",
    "                return clean, output.multi_hand_landmarks[0]  # Return both the clean data and landmarks for drawing\n",
    "            except:\n",
    "                # Return an array of zeros and None if no hand is detected\n",
    "                return np.zeros([1, 63], dtype=int)[0], None\n",
    "\n",
    "        # Load pre-trained SVM model\n",
    "        with open('model.pkl', 'rb') as f:\n",
    "            svm = pickle.load(f)\n",
    "\n",
    "        # Initialize webcam\n",
    "        cap = cv2.VideoCapture(0)\n",
    "\n",
    "        if not cap.isOpened():\n",
    "            print(\"Cannot open camera\")\n",
    "            exit()\n",
    "\n",
    "        while True:\n",
    "            # Capture frame-by-frame from webcam\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "                break\n",
    "\n",
    "            # Process the frame to get hand landmarks and the clean data\n",
    "            data, hand_landmarks = image_processed(frame)\n",
    "\n",
    "            # Convert data to numpy array\n",
    "            data = np.array(data)\n",
    "\n",
    "            # Predict the hand sign using the pre-trained SVM model\n",
    "            y_pred = svm.predict(data.reshape(-1, 63))\n",
    "            print(y_pred)\n",
    "\n",
    "            # Draw the MediaPipe landmarks on the frame if a hand is detected\n",
    "            if hand_landmarks:\n",
    "                # Draw landmarks and connections using MediaPipe's drawing utilities\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Font settings for the prediction text\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            org = (50, 100)\n",
    "            fontScale = 3\n",
    "            color = (255, 0, 0)  # Blue color\n",
    "            thickness = 5\n",
    "\n",
    "            # Display the predicted hand sign on the frame\n",
    "            frame = cv2.putText(frame, str(y_pred[0]), org, font, \n",
    "                                fontScale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "            # Show the frame with landmarks and predictions\n",
    "            cv2.imshow('Hand Sign Detection with MediaPipe', frame)\n",
    "\n",
    "            # Break the loop on 'q' key press\n",
    "            if cv2.waitKey(1) == ord('q'):\n",
    "                break\n",
    "\n",
    "        # Release the webcam and close windows\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Close the MediaPipe hands instance\n",
    "        hands.close()\n",
    "\n",
    "```\n",
    "\n",
    "**Link Video demo:** https://drive.google.com/file/d/1oXg2cvd-LwXO7xLRznvI59KoB1YpEK54/view?usp=sharing\n",
    "\n",
    "### Một số thứ bên lề\n",
    "\n",
    "Trong quá trình tìm hiểu Mediapipe thì em có tìm được một bài Github có nói về việc ghi lại ảnh động của ngôn ngữ ký hiệu sử dụng Mediapipe và DTW. Link Github: [Sign-Language-Recognition--MediaPipe-DTW](https://github.com/gabguerin/Sign-Language-Recognition--MediaPipe-DTW)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
