{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quá trình thực hiện"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Ngày bắt đầu | Công việc |                   Nội dung            | Tài liệu tham khảo |\n",
        "|:----|:------------------|:------------------|:-----------------------|\n",
        "|  05/09/2024  | Tìm hiểu về cách train một model AI |  Giữa nhiều lựa chọn khả thi như Anaconda, Jupyter, nhóm dùng Google Colab vì tiện dụng, dễ dùng, miễn phí và nhiều nguồn tham khảo | [Google Colab là gì](https://200lab.io/blog/google-colab-la-gi/) <br> [Làm quen với Google Colab](https://trituenhantao.io/lap-trinh/lam-quen-voi-google-colab/) <br> [Hướng dẫn tất tần tật về train model bằng Google Colab](https://www.youtube.com/watch?v=TEHnfsqpUPc) <br> [Train model sử dụng GPU trên Kaggle so với Colab?](https://www.youtube.com/watch?v=fXnrFGjSwrY)|\n",
        "|  05/09/2024   | Chọn phương án sử dụng để train sau khi tham khảo các phương án khả thi | Chọn sử dụng YOLOv8, train model trên Google Colab, sau khi đã thử YOLOv5 và Tensorflow. <br> Về phần dataset, ban đầu định sử dụng dữ liệu từ Kaggle, nhưng sau khi tìm được nguồn dữ liệu dễ dùng và đa dạng hơn đã chọn dùng từ Roboflow.<br>Chọn được 2 dataset để thực hiện train một model:<br>- Dataset 702 ảnh: [American Sign Language Letters Object Detection Dataset and Pre-Trained Model by David Lee](https://universe.roboflow.com/david-lee-d0rhs/american-sign-language-letters)<br>- Dataset 893 ảnh: [Dataset từ Roboflow](https://universe.roboflow.com/personnel-icnjh/signlanguage-pypvv) | [Copy of AndroidML-HandGestureRecognitionModelTraining.ipynb](https://github.com/mcdominik/machine_learning/blob/master/yolov8-on-fire-colab/yolo8_on_fire.ipynb)<br>[How to train a new model for gesture recognition - ML on Android with MediaPipe](https://www.youtube.com/watch?v=vODSFXEP-XY)<br>[Nhận diện vật thể với YOLOv7 và trên Google Colab (có sử dụng webcam)](https://www.youtube.com/watch?v=JzAA59wJtcg) <br> [YOLOv8 Webcam Object Detection](https://github.com/codershiyar/object-detection-using-webcam)<br>[Fire Detection](https://github.com/mcdominik/machine_learning/blob/master/yolov8-on-fire-colab/yolo8_on_fire.ipynb) |\n",
        "|  12/09/2024   |  Train thử model dùng dataset có sẵn   |  Thực hiện train model theo video [Real Time Sign Language Detection with Tensorflow Object Detection and Python Deep Learning SSD của Nicholas Renotte](https://www.youtube.com/watch?v=pDXdlXlaCco)<br>Sử dụng Google Colab train thành công, trong quá trình train có gặp lỗi đường dẫn `FileNotFoundError` trong file `data.yaml`, đã sửa thành công.<br>Phát sinh lỗi `TypeError: unhashable type: 'numpy.ndarray'` trong quá trình train, đã sửa thành công (Chuyển đổi `numpy.ndarray` thành `list`).<br>Quá trình train model 702 ảnh mất 10 phút, model 893 ảnh mất 1h30p (máy yếu). | [Real Time Sign Language Detection](https://www.youtube.com/watch?v=-RDeVPHipZU) |\n",
        "|  12/09/2024  | Chạy thử model đã train trên Visual Code, thực hiện nhận diện thử | Code mở được webcam và nhận diện đúng một số cử chỉ, song vẫn có sai sót trong qua trình nhận diện [Ảnh minh họa](https://drive.google.com/file/d/1WwpddPcOVpuZV3QP12tDkguknnd31gWZ/view?usp=sharing)<br> Sau khi chạy thử cả 2 model thành công, nhận thấy model 702 ảnh nhận diện tốt và chính xác hơn model 983 ảnh, chọn dùng model này làm mẫu | |\n",
        "|  19/09/2024  | Tự chụp dataset 302 ảnh chuẩn bị dữ liệu train | Thực hiện chụp một loạt ảnh các kí tự từ A đến Y cho cả hai tay trái và phải, mỗi chữ tầm 5 đến 10 bức <br>Tải tất cả 302 ảnh lên google drive [Ảnh tự train](https://drive.google.com/drive/folders/1-2ZbW4bG9pfgepF6Wbo49M2kOQAWpQhT?usp=sharing). <br>Trên máy tính, tải file về và nạp trên thiết lập dataset trên roboflow . <br>Thực hiện gắn label từng bức ảnh [Ảnh mi.nh họa](https://drive.google.com/file/d/18ZfZbNR38J3duaa153ucIYBYooU14WdL/view?usp=sharing) <br>Hoàn thành và xuất dataset hoàn chỉnh https: [Dataset](https://universe.roboflow.com/sign-language-m2gtx/sign-language-ngjvl)<br>Trong quá trình tải ảnh lên phát sinh lỗi ảnh chụp từ iphone cho ra file .HEIC không được roboflow hỗ trợ, phải chuyển tất cả lại thành JPG <br>Dùng chức năng tạo nhiều phiên bản khác nhau của ảnh nhằm tăng độ chính xác, tăng số lượng ảnh trong dataset lên 838 ảnh | [Manage Batches](https://docs.roboflow.com/datasets/manage-batches)<br> [Create a Dataset Version](https://docs.roboflow.com/datasets/create-a-dataset-version)|\n",
        "| 19/09/2024 | Dùng dataset tự chụp train model riêng | Train thành công dùng các bước cũ, train bằng google collab thành công và tiến hành chạy trên Visual Code, nhận diện được một số kí tự song vẫn chưa nhạy|  |\n",
        "| 19/09/2024 | Tìm hiểu về cách nhận diện chuyển động để model nhận diện được một số câu thông dụng như xin chào, cảm ơn, ... | |Tạp chí khoa học công nghệ thông tin và truyền thông : <br>[NHẬN DẠNG NGÔN NGỮ KÝ HIỆU TIẾNG VIỆT TRONG VIDEO BẰNG LSTM VÀ I3D ĐA KHỐI](https://jstic.ptit.edu.vn/jstic-ptit/index.php/jstic/article/view/303/149) <br>[IMPROVE CNN AND LSTM IN SENTIMENT ANALYSIS FOR VIETNAMESE FROM DATA PREPROCESSING PHASE](https://jstic.ptit.edu.vn/jstic-ptit/index.php/jstic/article/view/322/138) <br>[A STUDY ON PARAMETER TUNING FOROPTIMAL INDEXING ON LARGE SCALEDATASETS](https://jstic.ptit.edu.vn/jstic-ptit/index.php/jstic/article/view/294/148) <br>Tài liệu khác: [SỬ DỤNG MẠNG LSTM (LONG SHORT TERM MEMORY) ĐỂ DỰ ĐOÁN SỐ LIỆU HƯỚNG THỜI GIAN](https://trituevietvn.com/chi-tiet/su-dung-mang-lstm-long-short-term-memory-de-du-doan-so-lieu-huong-thoi-gian-123)|\n",
        "| 26/09/2024 | Cải thiện model nhận diện hình ảnh của tuần 3 với độ chính xác chưa cao bằng Mediapipe, cần nhận diện các khớp ngón tay trước, sau đó sẽ làm model dựa vạo vị trí khung tay để nhận diện chữ, nhờ vậy có thể nhận diện được chuyển động của tay cho kí tự Z đang còn thiếu | Xem video giới thiệu về Mediapipe của Google for Developer : [Applying computer vision - ML on Android with MediaPipe Series](https://www.youtube.com/watch?v=Odyg9furoQs) <br> Lưu github về Mediapipe của Google để dùng sau : [MediaPipe Hands](https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/hands.md) <br> Xem video thực hiện trên Youtube: [Custom Hand Gesture Recognition with Hand Landmarks Using Google’s Mediapipe + OpenCV in Python](https://www.youtube.com/watch?v=a99p_fAr6e4) | [Applying computer vision - ML on Android with MediaPipe Series](https://www.youtube.com/watch?v=Odyg9furoQs) | \n",
        "| 26/09/2024 | Chạy thử Mediapipe trên Visual Studio | Nhận diện được khung xương bàn tay tốt, độ chính xác cao | |\n",
        "| 3/10/2024 | Triển khai gắn label Keypoint Detection cho từng ảnh | Hoàn thành việc gắn skeleton label 302 ảnh vào ngày 8/10/2024 |  |\n",
        "| 9/10/2024 | Tìm hiểu về cách nhận diện sử dụng Mediapipe | Xem các video giới thiệu về mediapipe và cách sử dụng chúng | [American Sign Language Detection with Python and MediaPipe](https://www.youtube.com/watch?v=L-IaQch8KYY) <br> [Sign Language Detection Using Machine Learning](https://www.youtube.com/watch?v=Ui85SVJsRf8) <br> [Sign language detection with Python and Scikit Learn](https://www.youtube.com/watch?v=MJCSjXepaAM)| \n",
        "| 9/10/2024 | Thực hiện train model theo video Hands gesture recognition with mediapipe and svm classifier của AI4LIFE | Sau khi xem một vài video hướng dẫn nhận diện tay sử dụng Mediapipe thì thử thực hiện model với video hướng dẫn này. Kết quả thu được model nhận diện tay có độ chính xác còn thấp <br> File get_image.py dùng để thực hiện thu thập hình ảnh cho dataset với mỗi ký tự là 100 ảnh, 50 ảnh cho tay trái và 50 ảnh cho tay phải( trừ chữ A, B, C, D, E, F, G, H). Sau quá trình đó thì ta thu được một DATASET với 2600 ảnh <br> File get_data.py dùng để thực hiện label tự động cho mỗi ảnh để thu được các landmark của từng ảnh và được tập hợp lại thành file dataset.csv <br> File train.ipynb dùng để thực hiện train model là model.pkl <br>Sau khi có model thực hiện chạy wecam detect với file hands_gesture_recog.py. Vì cảm thấy thiếu Landmark Mediapipe khi chạy wecam để kiểm tra detect tay khi sử dụng Mediapipe thế nào nên đã lên chatGPT để thêm vào code hiện thị Landmark khi chạy wecam <br> **Link Video demo:** https://drive.google.com/file/d/1oXg2cvd-LwXO7xLRznvI59KoB1YpEK54/view?usp=sharing <br> Trong quá trình tìm hiểu Mediapipe tìm được một bài Github có nói về việc ghi lại ảnh động của ngôn ngữ ký hiệu sử dụng Mediapipe và DTW. **Link Github:** [Sign-Language-Recognition--MediaPipe-DTW](https://github.com/gabguerin/Sign-Language-Recognition--MediaPipe-DTW) | [Hands gesture recognition with mediapipe and svm classifier](https://www.youtube.com/watch?v=dlyVy_LNCEQ) <br> [hand-gesture-recognition](https://github.com/dongdv95/hand-gesture-recognition)|\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
